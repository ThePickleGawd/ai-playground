# Attention is All You Need

PyTorch implementation of "Attention is All You Need"

## Notes for me

Pipeline

- Tokenize text. Pass the encoding to model
- Context length is 1024
- We need an embedding (seems to be 768 per token)
- Feed to

Goals:

- Recreate GPT-2 without looking back at the Karpathy videos or github
